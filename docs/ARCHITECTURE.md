# Spider-Man Villain Timeline - Architecture

## System Overview

The Spider-Man Villain Timeline project follows the Context Engineering Protocol and is structured as a three-layer system:

```
┌─────────────────────────────────────────────────────────┐
│ Presentation Layer (Frontend)                           │
│ - D3.js visualization                                   │
│ - Interactive timeline graph                            │
│ - HTML5/CSS3 interface                                  │
└────────────┬────────────────────────────────────────────┘
             │
             │ (reads from)
             │
┌────────────▼────────────────────────────────────────────┐
│ Data Layer                                              │
│ - villains.json (normalized data)                       │
│ - Issue/antagonist relationships                        │
│ - Static JSON API                                       │
└────────────┬────────────────────────────────────────────┘
             │
             │ (generated by)
             │
┌────────────▼────────────────────────────────────────────┐
│ Processing Layer (Node.js/TypeScript)                   │
│ - Scraper: Extracts from Marvel Fandom                  │
│ - Parser: Converts HTML to structured data              │
│ - Processor: Normalizes and validates                   │
└─────────────────────────────────────────────────────────┘
```

## Component Architecture

### 1. Scraper Module (`src/scraper/`)

**Purpose**: Extract villain data from Marvel Fandom website

**Components**:

#### `marvelScraper.ts`
- Main scraper orchestrator
- Handles HTTP requests to Marvel Fandom
- Manages request rate limiting and error handling
- Coordinates issue-by-issue extraction

**Key Functions**:
```typescript
scrapeAmazingSpiderManVol1(startIssue: number, endIssue: number)
  → Promise<ScrapedData>

getIssueUrl(issueNumber: number) → string
```

**Data Flow**:
```
Issue Number → URL Generation
   ↓
HTTP Request → Marvel Fandom
   ↓
HTML Response → HTML Parser
   ↓
Parsed Structure → Villain Extractor
   ↓
Villain List → Normalization
```

#### `parser.ts`
- HTML parsing and extraction logic
- Uses Cheerio to select DOM elements
- Extracts antagonist section from issue pages
- Handles different HTML structures

**Key Functions**:
```typescript
parseAntagonistsFromHtml(html: string) → string[]

extractIssueInfo(html: string) → IssueInfo
```

### 2. Utilities Module (`src/utils/`)

#### `dataProcessor.ts`
- Normalizes villain names (removes duplicates, aliases)
- Structures data for D3.js consumption
- Validates data integrity
- Generates statistics

**Key Functions**:
```typescript
processVillainData(rawData: RawVillainData) → ProcessedData

normalizeVillainName(name: string) → string

generateVillainStats(data: ProcessedData) → VillainStats
```

**Normalization Rules**:
- Remove duplicate entries
- Standardize name formatting
- Handle aliases (e.g., "Green Goblin" / "Norman Osborn")
- Track first appearance

### 3. Visualization Module (`src/visualization/`)

#### `d3Graph.ts`
- Generates D3.js visualization configuration
- Creates data structure for graph rendering
- Exports visualization logic to frontend

**Key Functions**:
```typescript
generateD3Config(data: ProcessedData) → D3Config

formatDataForTimeline(data: ProcessedData) → TimelineData[]
```

### 4. Frontend (`public/`)

#### `index.html`
- Main HTML structure
- D3.js and utility script imports
- Visualization container

#### `script.js`
- Client-side D3.js rendering
- Interactive features (hover, filter, zoom)
- Event handling for user interactions

#### `style.css`
- Styling for timeline
- Responsive design
- Color scheme for villains

## Data Models

### Input Data Structure (from scraping)

```typescript
interface IssueData {
  issueNumber: number;
  title: string;
  publicationDate?: string;
  antagonists: string[];  // Raw villain names from page
}
```

### Processed Data Structure

```typescript
interface ProcessedVillain {
  id: string;                    // Unique identifier
  names: string[];               // Primary name + aliases
  firstAppearance: number;       // Issue number
  appearances: number[];         // All issues where appears
  frequency: number;             // Times appeared
}

interface TimelineData {
  issue: number;
  villains: ProcessedVillain[];
  villainCount: number;
}
```

### D3 Visualization Data

```typescript
interface D3DataPoint {
  issueNumber: number;
  villainsInIssue: string[];
  coordinates?: [number, number];
}

interface D3Config {
  data: D3DataPoint[];
  scales: {
    x: D3Scale;
    y: D3Scale;
  };
  colors: Map<string, string>;  // Villain → color mapping
}
```

## Data Flow Pipeline

```
1. SCRAPING PHASE
   Marvel Fandom URLs
        ↓
   HTTP Requests (Axios)
        ↓
   Raw HTML (per issue)
        ↓
   Cheerio Parsing
        ↓
   Extracted Antagonists List

2. PROCESSING PHASE
   Raw Villain Lists
        ↓
   Normalization
   (deduplicate, standardize names)
        ↓
   Structure Building
   (create villain records, track appearances)
        ↓
   Validation
   (ensure data consistency)
        ↓
   Statistics Generation

3. OUTPUT PHASE
   ProcessedData
        ↓
   JSON Serialization
        ↓
   villains.json (saved to disk)
        ↓
   Read by Frontend

4. VISUALIZATION PHASE
   villains.json
        ↓
   D3 Data Transform
        ↓
   Scale Generation
        ↓
   SVG Rendering
        ↓
   Interactive Timeline
```

## Error Handling Strategy

### Scraping Layer
- Network errors → Retry with exponential backoff
- Parse errors → Log and skip issue
- Missing data → Mark as "antagonists not found"

### Processing Layer
- Invalid data → Log warning, attempt recovery
- Normalization conflicts → Manual review flag

### Visualization Layer
- Missing data files → Display error message
- Invalid JSON → Render error state
- Empty datasets → Display "no data" message

## Extension Points

### Add a New Series (e.g., Ultimate Spider-Man)

1. Create new scraper method in `marvelScraper.ts`:
```typescript
scrapeUltimateSpiderMan(startIssue: number, endIssue: number)
```

2. Add series selection to CLI

3. Update visualization to handle multiple series

### Add Analysis Features

1. Add functions to `dataProcessor.ts`:
```typescript
getVillainTrends()
getEraAnalysis()
```

2. Expose via CLI commands

3. Visualize in frontend

### Add New Visualization Types

1. Create new module: `src/visualization/newVizType.ts`

2. Implement D3 rendering logic

3. Add toggle in frontend

## Performance Considerations

- **Scraping**: Sequential requests to avoid overwhelming server
- **Memory**: Process data in chunks for large datasets
- **D3 Rendering**: Use canvas for 1000+ data points
- **Caching**: Store scraped data to avoid repeated requests

## Security Notes

- No sensitive data handled
- Web scraping respects robots.txt
- No authentication required
- Data is static JSON only

## Context Engineering Integration

This project implements the Context Engineering Protocol:

### Tool Definitions
- **scraper**: Extract villain data
- **processor**: Normalize and validate
- **visualizer**: Generate D3 config

### Context Building
- User provides issue range
- System determines required tools
- Tools executed in dependency order

### Feedback Loop
- Scraping success rate tracked
- Data quality metrics recorded
- Visualization rendering performance monitored

See [Context Engineering Template](../../context-engineering-template/) for protocol details.
